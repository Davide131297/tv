name: Weekly Crawl Script

on:
  schedule:
    # Run every Monday at 2 AM UTC (0 2 * * 1)
    - cron: "0 2 * * 1"
  workflow_dispatch: # Allow manual triggering from GitHub UI

jobs:
  run-crawl:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"

      - name: Install dependencies
        run: |
          cd frontend
          npm install

      - name: Run crawl script
        run: |
          # Make API call with Bearer token
          CRAWL_URL="${{ secrets.DEPLOYMENT_URL }}/api/crawl/all"
          
          # Use a temporary file to store the response body
          RESPONSE_BODY=$(mktemp)

          http_code=$(curl --write-out "%{http_code}" \
            --silent --output "$RESPONSE_BODY" \
            -X POST \
            --header "Authorization: Bearer ${{ secrets.NEXT_PUBLIC_CRAWL_API_KEY }}" \
            "$CRAWL_URL")

          if [ "$http_code" -ne 200 ]; then
            echo "::error::Crawl API request failed with status code $http_code"
            echo "Response body:"
            cat "$RESPONSE_BODY"
            rm "$RESPONSE_BODY"
            exit 1
          else
            echo "Successfully triggered crawl script."
            rm "$RESPONSE_BODY"
          fi
